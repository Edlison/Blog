# NLP中处理文本方法

由于文本是一种非架构化的数据，我们要将这种非结构化的数据转化为结构化的信息，这样我们才能对其进行处理。

现在文本表示大致有三种方式。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-guanxi.png)



## One-Hot

**向量中每一个位置代表一个词**

**one-hot的缺点：**
- 无法表达词语之间的关系
- 这种过于稀疏的向量，导致计算和存储的效率不高



## 整数编码

**整数编码的缺点**
- 无法表达词语之间的关系
- 对于模型解释而言，整数编码可能具有挑战性



## Word Embedding (词嵌入)

优点：
- 可以将文本通过一个低维向量来表达，不像one-hot长
- 语义相似的词在向量空间上比较相近
- 通用性强，可用在不同的任务中



### Word2Vec

基于统计方法来获得词向量的方法。

两种训练模式：
- 通过上下文来预测当前词(CBOW)
- 通过当前词来预测上下文(Skip-gram)

提高速度的优化方法：
- Negtive Sample (负采样)
- Hierarchical Softmax

优点：
- 由于Word2Vec会考虑上下文，跟之前的Embedding方法相比，效果要更好
- 比之前的Embedding方法维度更少，速度更快
- 通用性强，用于各种NLP任务

**缺点：**
- 由于词和向量是一对一的关系，所以多义词的问题无法解决
- Word2Vec是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化



### GloVe

实现有三步：
- 根据语料库构建一个共现矩阵，矩阵中的每一个元素代表单词和上下文单词在特定大小的上下文窗口内共同出现的次数。
- 构建词向量和共现矩阵之间的近似关系。
- loss function



Reference

词嵌入 | Word embedding

https://easyai.tech/ai-definition/word-embedding/

Word2vec

https://easyai.tech/ai-definition/word2vec/

GloVe详解

http://www.fanyeong.com/2018/02/19/glove-in-detail/
